{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CC_Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Computational Creativity Project**\n",
        "\n",
        "---\n",
        "\n",
        "### **Generative Adversarial Networks for Image Generation**\n",
        "\n",
        "Saiham Rahman, Student ID: 210769593\n"
      ],
      "metadata": {
        "id": "xV7JrmETxrRX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeKkXoXzvePw",
        "collapsed": true,
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown #**Install libraries** \n",
        "# @markdown This cell will take a little while because it has to download several libraries.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "# !pip install --upgrade torch==1.9.1+cu111 torchvision==0.10.1+cu111 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "#!pip install --upgrade https://download.pytorch.org/whl/nightly/cu111/torch-1.11.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl https://download.pytorch.org/whl/nightly/cu111/torchvision-0.12.0.dev20211012%2Bcu111-cp37-cp37m-linux_x86_64.whl\n",
        "!git clone https://github.com/NVlabs/stylegan3\n",
        "# !git clone https://github.com/openai/CLIP\n",
        "# !pip install -e ./CLIP\n",
        "!pip install einops ninja\n",
        "\n",
        "import sys\n",
        "# sys.path.append('./CLIP')\n",
        "sys.path.append('./stylegan3')\n",
        "\n",
        "import io\n",
        "import os, time, glob\n",
        "import pickle\n",
        "import shutil\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "# import clip\n",
        "import unicodedata\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
        "from IPython.display import display\n",
        "from einops import rearrange\n",
        "from google.colab import files\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.utils import get_file\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "# from IPython import display\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training for StyleGAN3\n"
      ],
      "metadata": {
        "id": "qa3OfRPG5rSQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #**Uncomment to Connect google colab**\n",
        "def connect_gdrive():\n",
        "  try:\n",
        "      from google.colab import drive\n",
        "      drive.mount('/content/drive', force_remount=True)\n",
        "      COLAB = True\n",
        "      print(\"Note: using Google CoLab\")\n",
        "  except:\n",
        "      print(\"Note: not using Google CoLab\")\n",
        "      COLAB = False\n",
        "# connect_gdrive() #UNCOMMENT  "
      ],
      "metadata": {
        "cellView": "form",
        "id": "LV6dqtWMitYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown #**Loading Custom Data and Resizing Images**\n",
        "# @markdown Please uncomment the lines of code marked #UNCOMMENT to make it work\n",
        "from PIL import Image\n",
        "import os, sys\n",
        "#set file path\n",
        "path = \"/content/drive/MyDrive/images/\" #@param\n",
        "save_path = \"/content/drive/MyDrive/processed/\" #@param\n",
        "# dirs = os.listdir( path ) #UNCOMMENT\n",
        "\n",
        "SIZE = \"1024\" #@param [256, 1024]\n",
        "def resize():\n",
        "    \n",
        "    #loop through the images\n",
        "    for item in dirs:\n",
        "        if os.path.isfile(path+item):\n",
        "            im = Image.open(path+item)\n",
        "            f, e = os.path.splitext(path+item)\n",
        "            \n",
        "            #set the output size: 256,256 or 1024,1024\n",
        "            imResize = im.resize((SIZE,SIZE), Image.ANTIALIAS)\n",
        "            \n",
        "            #save the images as image name + resized\n",
        "            imResize.save(save_path + os.path.basename(item) + '_resized.jpg', 'JPEG', quality=100, optimize=True)\n",
        "#run the funtion\n",
        "# resize() #UNCOMMENT\n",
        "# !python /content/stylegan3/dataset_tool.py --source /content/drive/MyDrive/processed --dest /content/drive/MyDrive/metfaces #UNCOMMENT"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HiHucZqyifu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "#@markdown #**Training Loop for StyleGAN3**\n",
        "\n",
        "# @markdown #**Uncomment last line in code to initiate training**\n",
        "#@markdown Modify these according to https://github.com/NVlabs/stylegan3/blob/main/docs/configs.md\n",
        "EXPERIMENTS = \"/content/drive/MyDrive/experiments_metfaces\" #@param\n",
        "DATA = \"/content/drive/MyDrive/metfaces_128\" #@param\n",
        "SNAP = 10 #@param\n",
        "CONFIG = \"stylegan3-r\" #@param [\"stylegan3-r\", \"stylegan3-t\"]\n",
        "BATCH = 32 #@param \n",
        "GAMMA = 0.5 #@param\n",
        "MIRROR = 1 #@param\n",
        "KIMG =  5000#@param\n",
        "#@markdown *For loading papers pretrainned network on ffhq dataset*\n",
        "TRANSFER_LEARNING_RESUME = \"True\" #@param [\"True\", \"False\"]\n",
        "IMAGE_SIZE = \"1024\" #@param [256, 1024]\n",
        "TRANSFER_RESUME_NETWORK = \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-t-ffhqu-\"+str(SIZE)+\"x\"+str(SIZE)+\".pkl\"\n",
        "#@markdown ---\n",
        "#@markdown **For loading previously trainned network**\n",
        "PRETRAINNED_RESUME = \"False\" #@param [\"True\", \"False\"]\n",
        "NETWORK = \"network-snapshot-000000.pkl\" #@param\n",
        "NETWORK_FOLDER = \"00000--auto1\" #@param\n",
        "RESUME_NETWORK = os.path.join(EXPERIMENTS, \"00000--auto1\", NETWORK) \n",
        "\n",
        "# Build the command and run it\n",
        "if TRANSFER_LEARNING_RESUME == True and PRETRAINNED_RESUME == False:\n",
        "  cmd = f\"/usr/bin/python3 /content/stylegan3/train.py --outdir={EXPERIMENTS} --cfg={CONFIG} --data={DATA} --gpus=1 --batch={BATCH} --batch-gpu=4 --gamma={GAMMA} --mirror={MIRROR} --kimg={KIMG} --snap={SNAP} --resume={TRANSFER_RESUME_NETWORK}\"\n",
        "elif PRETRAINNED_RESUME == True:\n",
        "  cmd = f\"/usr/bin/python3 /content/stylegan3/train.py --outdir={EXPERIMENTS} --cfg={CONFIG} --data={DATA} --gpus=1 --batch={BATCH} --batch-gpu=4 --gamma={GAMMA} --mirror={MIRROR} --kimg={KIMG} --snap={SNAP} --resume={RESUME_NETWORK}\"\n",
        "else:\n",
        "  cmd = f\"/usr/bin/python3 /content/stylegan3/train.py --outdir={EXPERIMENTS} --cfg={CONFIG} --data={DATA} --gpus=1 --batch={BATCH} --batch-gpu=16 --gamma={GAMMA} --mirror={MIRROR} --kimg={KIMG} --snap={SNAP}\"\n",
        "\n",
        "\n",
        "# !{cmd} # EXECUTE TRAINING"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UVrDsidhi1Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and generating images using StyleGAN3"
      ],
      "metadata": {
        "id": "A0z-uX0i5tgd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Generate an image\n",
        "#@markdown StyleGAN3 pre-trained models for config T (translation equiv.) and config R (translation and rotation equiv.)\n",
        "\n",
        "OUTDIR = \"/content/seeds\"\n",
        "\n",
        "# Create directory\n",
        "try:\n",
        "    os.makedirs(OUTDIR)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "SEED_VALUE = 5088 #@param {type:\"slider\", min:1000, max:9999, step:1}\n",
        "\n",
        "baselink ='https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/'\n",
        "model = \"stylegan3-r-metfaces-1024x1024.pkl\" #@param [\"Trained Model\",\"stylegan3-r-metfaces-1024x1024.pkl\",\"stylegan3-r-metfacesu-1024x1024.pkl\",\"stylegan3-t-metfaces-1024x1024.pkl\",\"stylegan3-t-metfacesu-1024x1024.pkl\"]\n",
        "#@markdown Test with own model?\n",
        "OWN_MODEL = \"False\" #@param [\"True\", \"False\"]\n",
        "\n",
        "if OWN_MODEL == True:\n",
        "# Generate an image using own trained model\n",
        "  connect_gdrive()\n",
        "  NETWORK = \"network-snapshot-000000.pkl\" #@param\n",
        "  NETWORK_FOLDER = \"00000--auto1\" #@param\n",
        "  MODEL_PATH = os.path.join(\"/content/drive/MyDrive/\", \"00000--auto1\", NETWORK) \n",
        "  cmd = f\"/usr/bin/python3 /content/stylegan3/gen_images.py --outdir={OUTDIR} --trunc=1 --seeds={SEED_VALUE} --network={MODEL_PATH}\"\n",
        "  # !python gen_images.py --outdir=out --trunc=1 \\\n",
        "  # --seeds=$seed --network=$MODEL_PATH\n",
        "else:\n",
        "# Generate an image using pre-trained model\n",
        "  cmd = f\"/usr/bin/python3 /content/stylegan3/gen_images.py --outdir={OUTDIR} --trunc=1 --seeds={SEED_VALUE} --network={baselink+model}\"\n",
        "  # !python gen_images.py --outdir=out --trunc=1 \\\n",
        "  # --seeds=$seed --network=$baselink$model\n",
        "\n",
        "!{cmd}\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "gan_img = Image.open(f'{OUTDIR}/seed%04d.png' % SEED_VALUE);\n",
        "gan_img.save(f'{OUTDIR}/seed%04d.png' % SEED_VALUE) # Save it\n",
        "plt.imshow(gan_img);\n",
        "plt.axis('off');"
      ],
      "metadata": {
        "id": "dtf6HQMR4A9B",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# StyleTransfer"
      ],
      "metadata": {
        "id": "hPUqU8K9dGdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown #**Define necessary functions for StyleTransfer**\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import get_file\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from keras import Model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "#from keras.optimizers import SGD\n",
        "from IPython import display\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "network_choice = \"vgg16\" #@param['vgg19', 'vgg16']\n",
        "\n",
        "switcher1 = {\n",
        "    \"vgg19\": applications.vgg19,\n",
        "    \"vgg16\": applications.vgg16,\n",
        "}\n",
        "switcher2 = {\n",
        "    \"vgg19\": applications.vgg19.VGG19(weights=\"imagenet\", include_top=False),\n",
        "    \"vgg16\": applications.vgg16.VGG16(weights=\"imagenet\", include_top=False),\n",
        "}\n",
        "network = switcher1[network_choice]\n",
        "\n",
        "def build_model():\n",
        "    model = switcher2[network_choice]\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# Display images function\n",
        "\n",
        "def display_images(content_image, style_image, combination_image):\n",
        "    \"\"\"\n",
        "    Displays content, style and combination images, labelled. Any could be missing if set to None\n",
        "    \"\"\"\n",
        "    _, cells = plt.subplots(1, 3, figsize=(45,15))\n",
        "    if content_image is not None:\n",
        "        cells[0].imshow(content_image)\n",
        "        cells[0].set_title(\"Content\", fontsize=30)\n",
        "    if style_image is not None:\n",
        "        cells[1].imshow(style_image)\n",
        "        cells[1].set_title(\"Style\", fontsize=30)\n",
        "    if combination_image is not None:\n",
        "        cells[2].imshow(combination_image)\n",
        "        cells[2].set_title(\"Combination\", fontsize=30)\n",
        "    for cell in cells:\n",
        "        cell.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    \"\"\"\n",
        "    Open, resize and format pictures into appropriate tensors\n",
        "    \"\"\"\n",
        "    # Load image with given size into PIL format\n",
        "    img = keras.preprocessing.image.load_img(\n",
        "        image_path, target_size=(img_height,img_width)\n",
        "    )  \n",
        "    # Turn image into Numpy array\n",
        "    img = keras.preprocessing.image.img_to_array(img)\n",
        "    # The next step expects a batch of images, so add another dimension\n",
        "    img = np.expand_dims(img, axis=0)  \n",
        "    # Call the network preprocessing step. \n",
        "    # Converts to BGR and zero-centers data with regards to the Imagenet dataset, by subtracting the mean channel values in Imagenet\n",
        "    img = network.preprocess_input(img)  \n",
        "    # Transform to tensor\n",
        "    return tf.convert_to_tensor(img)  \n",
        "\n",
        "def deprocess_image(tensor):\n",
        "    \"\"\"\n",
        "    Reverse the preprocessing step to turn the tensor into an RGB Numpy array which we can visualise\n",
        "    \"\"\"\n",
        "    # Transform the tensor into a Numpy array\n",
        "    img = tensor.numpy()[0]\n",
        "\n",
        "    # Return BGR values to normal (non-zero-centered), adding back in the means of the Imagenet dataset\n",
        "    img[:, :, 0] += 103.939\n",
        "    img[:, :, 1] += 116.779\n",
        "    img[:, :, 2] += 123.68\n",
        "\n",
        "    # Convert from BGR to RGB\n",
        "    img = img[:, :, ::-1]\n",
        "\n",
        "    # Ensure values are in valid ranges\n",
        "    img = np.clip(img, 0, 255).astype(\"uint8\")\n",
        "\n",
        "    return img\n",
        "\n",
        "# Loss function\n",
        "\n",
        "def gram_matrix(input_tensor):\n",
        "    \"\"\"\n",
        "    Calculate the Gram matrix for given input\n",
        "    \"\"\"\n",
        "    input_tensor = tf.transpose(input_tensor, (2, 0, 1))  # Transpose\n",
        "    features = tf.reshape(input_tensor, (tf.shape(input_tensor)[0], -1))  # Flatten layer\n",
        "    gram = tf.matmul(features, tf.transpose(features))\n",
        "    return gram\n",
        "\n",
        "def style_loss(style, combination):\n",
        "    \"\"\"\n",
        "    The style loss is the mean square error between the Gram matrix of the style image features and the Gram matrix of the combination image features,\n",
        "    divided by 4 in original paper (see link at the end of the notebook)\n",
        "    \"\"\"\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    return tf.reduce_sum(tf.square(S - C)) / (4.0 * (channels ** 2) * (img_size ** 2))\n",
        "\n",
        "def content_loss(content, combination):\n",
        "    \"\"\"\n",
        "    The content loss is the mean square error between the combination and the content image features\n",
        "    \"\"\"\n",
        "    return tf.reduce_sum(tf.square(combination - content))\n",
        "\n",
        "def loss_function(combination_image, content_image, style_image):\n",
        "    \"\"\"\n",
        "    Calculate the loss function given a content image, a style image and the combination result\n",
        "    \"\"\"\n",
        "    # Combine all the images in the same tensor\n",
        "    input_tensor = tf.concat(\n",
        "        [content_image, style_image, combination_image], axis=0\n",
        "    )\n",
        "\n",
        "    # Get the features in all the layers for the three images\n",
        "    features = feature_extractor(input_tensor)\n",
        "\n",
        "    # Initialise the loss\n",
        "    loss = tf.zeros(shape=())\n",
        "\n",
        "    # Extract the content layers and calculate the content loss\n",
        "    for layer_name in content_layers:\n",
        "        layer_features = features[layer_name]\n",
        "        content_image_features = layer_features[0, :, :, :]  # Content image is at position 0\n",
        "        combination_image_features = layer_features[2, :, :, :]  # Combination image is at position 2\n",
        "        loss += content_weight * content_loss(content_image_features, combination_image_features)\n",
        "\n",
        "    # Extract the style layers and calculate the style loss\n",
        "    for layer_name in style_layers:\n",
        "        layer_features = features[layer_name]\n",
        "        style_image_features = layer_features[1, :, :, :]  # Style image is at position 1\n",
        "        combination_image_features = layer_features[2, :, :, :]\n",
        "        loss += style_weight * style_loss(style_image_features, combination_image_features)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def compute_loss_and_grads(content_image, style_image, combination_image):\n",
        "    \"\"\"\n",
        "    Brings together the calculation of loss and that of gradients\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = loss_function(combination_image, content_image, style_image)\n",
        "        grads = tape.gradient(loss, combination_image)\n",
        "    return loss, grads\n",
        "\n",
        "def save(epoch, img_tensor, save_image=True, save_model=False):\n",
        "  drive.mount('/content/gdrive')\n",
        "  # Directory where the checkpoints will be saved\n",
        "  path = '/content/gdrive/My Drive/Work/Colab/StyleTransfer/' #@param{type: 'string'}\n",
        "\n",
        "  model_name = path + \"model_\" + str(epoch) + '.tf'\n",
        "  image_name = path + \"img_\" + str(epoch) + '.png'\n",
        "\n",
        "  # Save image\n",
        "  if save_image:\n",
        "      img = deprocess_image(img_tensor)\n",
        "      keras.preprocessing.image.save_img(image_name, img)\n",
        "\n",
        "  # Save model\n",
        "  if save_model:\n",
        "      tf.saved_model.save(model, model_name)\n"
      ],
      "metadata": {
        "id": "Ws_Wb-fyK-7q",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load chosen images and preprocess\n",
        "\n",
        "style_image_url = \"https://i.imgur.com/osgpBSch.jpg\" # @param {type: \"string\"}\n",
        "GAN_IMAGE_FILENAME = f\"{SEED_VALUE}\" #@param [\"f\\\"{SEED_VALUE}\\\"\", \"\\\"seed4011\\\"\"] {type:\"raw\", allow-input: true}\n",
        "content_image_path = \"/content/seeds/seed\"+ GAN_IMAGE_FILENAME +\".png\"\n",
        "\n",
        "style_image_path = get_file(fname = \"style_image.jpg\", origin = style_image_url) # TODO\n",
        "\n",
        "content_image = keras.preprocessing.image.load_img(content_image_path)\n",
        "style_image = keras.preprocessing.image.load_img(style_image_path)\n",
        "\n",
        "# display\n",
        "display_images(content_image, style_image, None)\n",
        "\n",
        "#@title Image dimensions\n",
        "\n",
        "# Turn images into Numpy arrays to extract full dimensions\n",
        "content_image_np = keras.preprocessing.image.img_to_array(content_image)\n",
        "style_image_np = keras.preprocessing.image.img_to_array(style_image)\n",
        "\n",
        "# Setting image dimensions variables\n",
        "height = content_image_np.shape[0]\n",
        "width = content_image_np.shape[1]\n",
        "channels = content_image_np.shape[2]\n",
        "# Set height\n",
        "img_height = 400  #@param{type: 'number'}\n",
        "# Scale width appropriately\n",
        "img_width = int(width * img_height / height) \n",
        "img_size = img_height * img_width\n",
        "\n",
        "#@title Test preprocessing\n",
        "tensor = preprocess_image(style_image_path)\n",
        "img = deprocess_image(tensor)"
      ],
      "metadata": {
        "id": "9VOFqT61LC79",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Build the model\n",
        "model = build_model()\n",
        "\n",
        "# Set up a model to extract features, given input, for each layer in the network\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "feature_extractor = Model(inputs=model.inputs, outputs=outputs_dict)\n"
      ],
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "v8YTXYGPLTRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Define the optimizer\n",
        "\n",
        "initial_learning_rate = 100.0 #@param\n",
        "decay_steps = 100 #@param\n",
        "decay_rate = 0.96 #@param\n",
        "\n",
        "optimizer = SGD(tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=initial_learning_rate, decay_steps=decay_steps, decay_rate=decay_rate))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Wg12AnUBLYBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Set which layers of the network should be taken into consideration when calculating content and style loss\n",
        "style_layers = [\n",
        "    \"block1_conv1\",\n",
        "    \"block2_conv1\",\n",
        "    \"block3_conv1\",\n",
        "    \"block4_conv1\",\n",
        "    \"block5_conv1\",\n",
        "]\n",
        "content_layers = [\"block5_conv2\"]\n",
        "\n",
        "# Total loss is a weighted sum of content and style loss\n",
        "content_weight = 2.5e-8  #@param\n",
        "# style_weight = 1e-6\n",
        "style_weight = 1e-6  #@param\n",
        "content_weight /= len(content_layers)\n",
        "style_weight /= len(style_layers)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "JJjgUwduLhEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Compile the Model\n",
        "# Compile the model\n",
        "model.compile(optimizer, loss_function)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "LtSdi2DgLh2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Optimization loop\n",
        "style_transfer_save_path = f\"/content/styletransfer/seed{SEED_VALUE}/\"\n",
        "import os\n",
        "try:\n",
        "    os.makedirs(style_transfer_save_path)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "content_image_tensor = preprocess_image(content_image_path)\n",
        "style_image_tensor = preprocess_image(style_image_path)\n",
        "combination_image_tensor = tf.Variable(preprocess_image(content_image_path))\n",
        "\n",
        "iterations =   500#@param{type: 'number'}\n",
        "\n",
        "for it in range(0, iterations + 1):\n",
        "    # Step 1: calculate loss and gradients\n",
        "    loss, grads = compute_loss_and_grads(content_image_tensor, style_image_tensor, combination_image_tensor)\n",
        "    # Step 2: apply gradients\n",
        "    optimizer.apply_gradients([(grads, combination_image_tensor)])\n",
        "    \n",
        "    # Display images\n",
        "    # display.clear_output()  # This line clears output between iterations\n",
        "    if it % 100 == 0:\n",
        "      print(\"Iteration %d: loss=%.2f\" % (it, loss))\n",
        "      # display_images(content_image, style_image, deprocess_image(combination_image_tensor))\n",
        "      # display_images(None,deprocess_image(combination_image_tensor),None)\n",
        "      keras.preprocessing.image.save_img(style_transfer_save_path + f\"seed{SEED_VALUE}_iter%d.png\"%(it), deprocess_image(combination_image_tensor))\n",
        "    if it == iterations:\n",
        "      display_images(content_image, style_image, deprocess_image(combination_image_tensor))\n",
        "\n",
        "    # Save image and/or model\n",
        "    #save(it, combination_image_tensor, save_image=False, save_model=False)"
      ],
      "metadata": {
        "id": "gFZFsuI7Ln71",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Using Tensorflow HUB\n",
        "import tensorflow_hub as hub\n",
        "hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "# Using the numpy arrays representing our images, with 1 batch dimension added, and normalized\n",
        "c_img = np.expand_dims(content_image_np, axis=0)/255 \n",
        "s_img = np.expand_dims(style_image_np, axis=0)/255 \n",
        "\n",
        "stylized_image = hub_model(tf.constant(c_img), tf.constant(s_img))[0]\n",
        "display_images(c_img[0], s_img[0], stylized_image[0])\n",
        "keras.preprocessing.image.save_img(style_transfer_save_path + f\"seed{SEED_VALUE}_TF_HUB.png\", stylized_image[0])"
      ],
      "metadata": {
        "id": "xfdNOt-JUwKZ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MISCALANIOUS Codes"
      ],
      "metadata": {
        "id": "4XKFVd_o28Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code for making collage\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# # open images by providing path of images\n",
        "# img1 = Image.open(\"/content/fake_tot/seed2363_TF_HUB.png\") \n",
        "# img2 = Image.open(\"/content/fake_tot/seed3676_TF_HUB.png\")\n",
        "# img3 = Image.open(\"/content/fake_tot/seed4089_TF_HUB.png\")\n",
        "# img4 = Image.open(\"/content/fake_tot/seed5088_TF_HUB.png\")\n",
        "# img5 = Image.open(\"/content/fake_tot/seed6463_TF_HUB.png\")\n",
        "# img6 = Image.open(\"/content/fake_tot/seed8001_TF_HUB.png\")\n",
        "# #create arrays of above images\n",
        "# img1_array = np.array(img1)\n",
        "# img2_array = np.array(img2)\n",
        "# img3_array = np.array(img3)\n",
        "# img4_array = np.array(img4)\n",
        "# img5_array = np.array(img5)\n",
        "# img6_array = np.array(img6)\n",
        "# # ====== collage of 4 images ====== \n",
        "# # arrange arrays of four images in two rows\n",
        "# imgg1 = np.vstack([np.hstack([img1_array , img2_array, img3_array]) , np.hstack([img4_array , img5_array, img6_array])]) \n",
        "# #create image of imgg1 array\n",
        "# finalimg1 = Image.fromarray(imgg1)\n",
        "# #provide the path with name for finalimg1 where you want to save it\n",
        "# finalimg1.save(\"/content/collage.png\")\n",
        "# print(\"Second image saved\")"
      ],
      "metadata": {
        "id": "5JZHdlpWK5PG",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}